import os
import sys
import time
import re
import json
import concurrent.futures
from datetime import datetime
from typing import Dict, Any, Tuple, List
from llm_wrapper import LLMService
from image_gen import ImageGenService
import config
import md_parser

class DesignWorkflow:
    def __init__(self, output_dir=None, custom_config=None):
        self.custom_config = custom_config or {}
        # ä¼˜å…ˆä½¿ç”¨ custom_config ä¸­çš„ api_key
        api_key = self.custom_config.get('OPENAI_API_KEY')
        base_url = self.custom_config.get('OPENAI_BASE_URL')
        
        self.llm = LLMService(api_key=api_key, base_url=base_url)
        
        # åˆå§‹åŒ–ç»˜å›¾æœåŠ¡
        jimeng_script = self.custom_config.get('JIMENG_SERVER_SCRIPT') or config.JIMENG_SERVER_SCRIPT
        self.image_gen = ImageGenService(server_script_path=jimeng_script)
        
        self.history = []
        self.generated_images = []
        
        self.output_dir = output_dir or config.OUTPUT_DIR
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

        # åŠ è½½å¤–éƒ¨çŸ¥è¯†åº“
        self.knowledge_base = self._load_knowledge_base()

    def _load_knowledge_base(self):
        """
        åŠ è½½ KNOWLEDGE.md å†…å®¹
        """
        kb_path = "KNOWLEDGE.md"
        if os.path.exists(kb_path):
            with open(kb_path, 'r', encoding='utf-8') as f:
                print(f"ğŸ“š å·²åŠ è½½å¤–éƒ¨çŸ¥è¯†åº“: {kb_path}")
                return f.read()
        else:
            print("âš ï¸ æœªæ‰¾åˆ°çŸ¥è¯†åº“æ–‡ä»¶ KNOWLEDGE.md")
            return "æš‚æ— å¤–éƒ¨çŸ¥è¯†åº“ã€‚"

    def log(self, message):
        print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")

    def run(self, product_brief: str):
        self.log(f"ğŸš€ å¯åŠ¨ AI è®¾è®¡å·¥ä½œæµï¼Œç›®æ ‡ä»»åŠ¡: {product_brief}")
        
        # Step 1: å¸‚åœºä¸ç«å“åˆ†æ
        self.log("ğŸ” Agent 1 (Market Analyst) æ­£åœ¨è¿›è¡Œå¸‚åœºè¶‹åŠ¿åˆ†æ...")
        market_analysis, _ = self.step_market_analysis(product_brief)
        self.log("âœ… å¸‚åœºåˆ†æå®Œæˆ")
        self._save_intermediate("1_Market_Analysis.md", market_analysis)

        # Step 2: è§†è§‰å‚è€ƒä¸ç—›ç‚¹æŒ–æ˜
        self.log("ğŸ¨ Agent 2 (Visual Researcher) æ­£åœ¨å¯»æ‰¾è§†è§‰å‚è€ƒå¹¶åˆ†æç—›ç‚¹...")
        visual_research, _ = self.step_visual_research(product_brief, market_analysis)
        self.log("âœ… è§†è§‰è°ƒç ”å®Œæˆ")
        self._save_intermediate("2_Visual_Research.md", visual_research)

        # Step 3: æ–¹æ¡ˆç”Ÿæˆä¸ Prompt è¾“å‡º
        self.log("ğŸ’¡ Agent 3 (Product Designer) æ­£åœ¨æ„æ€è®¾è®¡æ–¹æ¡ˆä¸ç»˜å›¾ Prompt...")
        design_proposals, design_prompts = self.step_design_generation(product_brief, market_analysis, visual_research)
        self.log("âœ… è®¾è®¡æ–¹æ¡ˆç”Ÿæˆå®Œæˆ")
        self._save_intermediate("3_Design_Proposals.md", design_proposals)

        # Step 4: è°ƒç”¨å³æ¢¦ç”Ÿæˆå›¾ç‰‡
        self.log("ğŸ¨ Agent 4 (Image Generator) æ­£åœ¨æ ¹æ®æ–¹æ¡ˆç”Ÿæˆæ¦‚å¿µå›¾...")
        self.step_image_generation(design_prompts)
        self.log("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆ")

        # Step 5: ç”ŸæˆæŠ¥å‘Š
        report_path = self._save_report(product_brief, market_analysis, visual_research, design_proposals)
        self.log(f"ğŸ“„ å®Œæ•´è®¾è®¡æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

    def _get_prompt(self, agent_name, default_template, **kwargs):
        """
        è·å– Promptï¼Œä¼˜å…ˆä½¿ç”¨ CONFIG.md ä¸­çš„é…ç½®
        """
        prompts = self.custom_config.get('prompts', {})
        template = prompts.get(agent_name, default_template)
        
        # è‡ªåŠ¨æ³¨å…¥ knowledge å‚æ•°ï¼Œå¦‚æœæ¨¡æ¿ä¸­æœ‰ {knowledge} å ä½ç¬¦
        if "{knowledge}" in template and "knowledge" not in kwargs:
            kwargs["knowledge"] = self.knowledge_base

        try:
            return template.format(**kwargs)
        except KeyError as e:
            self.log(f"âš ï¸ Prompt æ¨¡æ¿å‚æ•°ç¼ºå¤±: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡æ¿")
            return default_template.format(**kwargs)

    def _process_llm_json_response(self, raw_response: str) -> Tuple[str, List[Dict]]:
        """
        è§£æ LLM çš„ JSON å“åº”ï¼Œç”Ÿæˆæ’å›¾ï¼Œå¹¶è¿”å›æ ¼å¼åŒ–çš„ Markdown å’Œæ•°æ®åˆ—è¡¨
        """
        try:
            # å°è¯•æå– JSON å—
            json_str = raw_response
            match = re.search(r"```json\s*(.*?)```", raw_response, re.DOTALL)
            if match:
                json_str = match.group(1)
            else:
                # å°è¯•æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
                match = re.search(r"\{.*\}", raw_response, re.DOTALL)
                if match:
                    json_str = match.group(0)
            
            data = json.loads(json_str)
            
            summary = data.get("summary", "")
            content = data.get("content", "")
            visuals = data.get("visuals", [])
            prompts = data.get("prompts", []) # For step 3
            
            # 1. ç»„åˆ Summary
            final_content = ""
            if summary:
                final_content += f"> ğŸ’¡ **æ ¸å¿ƒæ‘˜è¦**: {summary}\n\n"
            
            # 2. ç”Ÿæˆå¹¶æ’å…¥æ’å›¾ (Visuals)
            if visuals:
                self.log(f"    - æ£€æµ‹åˆ° {len(visuals)} ä¸ªå¯è§†åŒ–æ¦‚å¿µï¼Œå‡†å¤‡ç”Ÿæˆæ’å›¾...")
                for item in visuals:
                    concept = item.get("concept", "Concept")
                    prompt = item.get("prompt", "")
                    if prompt:
                        self.log(f"      -> ç”Ÿæˆæ’å›¾: {concept}...")
                        img_path = self.image_gen.generate_image(prompt, self.output_dir)
                        if img_path:
                            rel_path = os.path.basename(img_path)
                            final_content += f"\n![{concept}]({rel_path})\n*å›¾ç¤ºï¼š{concept}*\n"
                            self.generated_images.append(img_path)
            
            final_content += content
            
            return final_content, prompts if prompts else visuals
            
        except json.JSONDecodeError:
            self.log("âš ï¸ æ— æ³•è§£æ LLM çš„ JSON å“åº”ï¼Œå°†è¿”å›åŸå§‹æ–‡æœ¬ã€‚")
            return raw_response, []
        except Exception as e:
            self.log(f"âš ï¸ å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            return raw_response, []

    def step_market_analysis(self, brief) -> Tuple[str, List]:
        default_prompt = "è¯·è¾“å‡º JSON æ ¼å¼çš„å¸‚åœºåˆ†æã€‚" # Fallback
        prompt = self._get_prompt('market_analyst', default_prompt, brief=brief, knowledge=self.knowledge_base)
        messages = [{"role": "user", "content": prompt}]
        response = self.llm.chat_completion(messages)
        return self._process_llm_json_response(response)

    def step_visual_research(self, brief, market_analysis) -> Tuple[str, List]:
        default_prompt = "è¯·è¾“å‡º JSON æ ¼å¼çš„è§†è§‰è°ƒç ”ã€‚" # Fallback
        prompt = self._get_prompt('visual_researcher', default_prompt, brief=brief, market_analysis=market_analysis, knowledge=self.knowledge_base)
        messages = [{"role": "user", "content": prompt}]
        response = self.llm.chat_completion(messages)
        return self._process_llm_json_response(response)

    def step_design_generation(self, brief, market_analysis, visual_research) -> Tuple[str, List]:
        default_prompt = "è¯·è¾“å‡º JSON æ ¼å¼çš„è®¾è®¡æ–¹æ¡ˆã€‚" # Fallback
        prompt = self._get_prompt('product_designer', default_prompt, brief=brief, market_analysis=market_analysis, visual_research=visual_research, knowledge=self.knowledge_base)
        messages = [{"role": "user", "content": prompt}]
        response = self.llm.chat_completion(messages)
        return self._process_llm_json_response(response)

    def step_image_generation(self, prompts_list: List[Dict]):
        """
        æ ¹æ® Prompts åˆ—è¡¨ç”Ÿæˆå›¾ç‰‡ (å¹¶è¡Œ)
        """
        if not prompts_list:
            self.log("    âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆ Promptï¼Œè·³è¿‡ç»˜å›¾ã€‚")
            return

        # æå– prompt æ–‡æœ¬
        clean_prompts = []
        for item in prompts_list:
            p = item.get("prompt", "")
            if p:
                clean_prompts.append(p)
        
        if not clean_prompts:
            return

        total = len(clean_prompts)
        self.log(f"    - å‡†å¤‡å¹¶è¡Œç”Ÿæˆ {total} å¼ æ–¹æ¡ˆå›¾...")
        
        def generate_single(p):
            try:
                return self.image_gen.generate_image(p, self.output_dir)
            except Exception as e:
                self.log(f"Error generating image: {e}")
                return None

        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(generate_single, p) for p in clean_prompts]
            for future in concurrent.futures.as_completed(futures):
                img_path = future.result()
                if img_path:
                    self.log(f"      -> å›¾ç‰‡å·²ä¿å­˜: {os.path.basename(img_path)}")
                    self.generated_images.append(img_path)

    # ä¿ç•™æ—§çš„å†…éƒ¨æ–¹æ³•åä»¥å…¼å®¹ï¼ˆå¦‚æœæœ‰å…¶ä»–åœ°æ–¹è°ƒç”¨ï¼‰ï¼Œä½†æŒ‡å‘æ–°æ–¹æ³•
    _step_market_analysis = step_market_analysis
    _step_visual_research = step_visual_research
    _step_design_generation = step_design_generation

    # æ—§çš„ _step_image_generation é€»è¾‘ä¸å†é€‚ç”¨æ–°çš„ JSON ç»“æ„ï¼Œ
    # ä½†ä¸ºäº†å…¼å®¹ web_app å¯èƒ½çš„è°ƒç”¨ï¼Œæˆ‘ä»¬éœ€è¦ä¿ç•™ä¸€ä¸ªé€‚é…å™¨
    def _step_image_generation(self, design_proposals):
        # å¦‚æœ web_app è¿˜åœ¨ä¼ æ–‡æœ¬è¿›æ¥ï¼Œæˆ‘ä»¬å°è¯•ç”¨æ—§é€»è¾‘æ­£åˆ™æå–?
        # æˆ–è€… web_app åº”è¯¥æ›´æ–°ä¸ºä¼ é€’ prompts_list
        # è¿™é‡Œå…ˆä¿ç•™æ—§é€»è¾‘ä½œä¸º fallbackï¼Œæˆ–è€…æ‰“å°è­¦å‘Š
        pass

    def _save_intermediate(self, filename, content):
        filepath = os.path.join(self.output_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

    def _save_report(self, brief, p1, p2, p3):
        filename = f"Full_Design_Report.md"
        filepath = os.path.join(self.output_dir, filename)
        
        content = f"""# AI è®¾è®¡å·¥ä½œæµæŠ¥å‘Š

## é¡¹ç›®éœ€æ±‚
{brief}

## ç¬¬ä¸€é˜¶æ®µï¼šå¸‚åœºåˆ†æ
{p1}

## ç¬¬äºŒé˜¶æ®µï¼šè§†è§‰è°ƒç ”ä¸ç—›ç‚¹åˆ†æ
{p2}

## ç¬¬ä¸‰é˜¶æ®µï¼šè®¾è®¡æ–¹æ¡ˆä¸ Prompts
{p3}

---
*Generated by AI Design Workflow at {datetime.now()}*
"""
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        return filepath

def main():
    # 1. è¯»å–é…ç½®
    config_path = "CONFIG.md"
    custom_config = {}
    if os.path.exists(config_path):
        print(f"ğŸ“– è¯»å–ç³»ç»Ÿé…ç½®: {config_path}")
        custom_config = md_parser.parse_config_md(config_path)
    else:
        print("âš ï¸ æœªæ‰¾åˆ° CONFIG.mdï¼Œä½¿ç”¨é»˜è®¤é…ç½®")

    # 2. è¯»å–éœ€æ±‚
    request_path = "REQUEST.md"
    if len(sys.argv) > 1:
        # æ”¯æŒå‘½ä»¤è¡Œè¦†ç›–
        brief = sys.argv[1]
        project_name = f"manual_run_{int(time.time())}"
    elif os.path.exists(request_path):
        print(f"ğŸ“– è¯»å–ç”¨æˆ·éœ€æ±‚: {request_path}")
        req_data = md_parser.parse_request_md(request_path)
        brief = req_data['description']
        project_name = req_data['project_name']
        
        if not brief:
            print("âŒ é”™è¯¯: REQUEST.md ä¸­æœªæ‰¾åˆ°è¯¦ç»†éœ€æ±‚æè¿°ã€‚")
            return
    else:
        print("è¯·è¾“å…¥è®¾è®¡éœ€æ±‚ (ä¾‹å¦‚: 'è®¾è®¡ä¸€æ¬¾ä¸­é«˜ç«¯çš„å®æœ¨ç›¸æ¡†')")
        brief = input("> ")
        project_name = f"manual_run_{int(time.time())}"

    # 3. åˆ›å»ºé¡¹ç›®æ–‡ä»¶å¤¹
    # å¤„ç†ä¸åˆæ³•çš„æ–‡ä»¶åå­—ç¬¦
    safe_project_name = "".join([c for c in project_name if c.isalpha() or c.isdigit() or c in (' ', '_', '-')]).strip()
    safe_project_name = safe_project_name.replace(' ', '_')
    
    project_dir = os.path.join("projects", safe_project_name)
    if not os.path.exists(project_dir):
        os.makedirs(project_dir)
        print(f"ğŸ“‚ åˆ›å»ºé¡¹ç›®æ–‡ä»¶å¤¹: {project_dir}")
    else:
        print(f"ğŸ“‚ ä½¿ç”¨å·²æœ‰é¡¹ç›®æ–‡ä»¶å¤¹: {project_dir}")
    
    # 4. è¿è¡Œå·¥ä½œæµ
    workflow = DesignWorkflow(output_dir=project_dir, custom_config=custom_config)
    workflow.run(brief)

if __name__ == "__main__":
    main()
