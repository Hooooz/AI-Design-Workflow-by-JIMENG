import os
import sys
import time
import json
from datetime import datetime
from typing import List, Dict, Any

# Add current directory to path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

import config


class RateLimitError(Exception):
    """é€Ÿç‡é™åˆ¶é”™è¯¯ï¼Œéœ€è¦åˆ‡æ¢æ¨¡å‹"""

    def __init__(self, message="Rate limit exceeded", retry_after: int = None):
        self.message = message
        self.retry_after = retry_after
        super().__init__(message)


class LLMService:
    def __init__(self, api_key=None, base_url=None):
        self.api_key = api_key or config.OPENAI_API_KEY
        self.base_url = base_url or config.OPENAI_BASE_URL
        self.client = None

        # Initialize logs directory
        self.log_dir = os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "logs"
        )
        if not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir)
        self.log_file = os.path.join(self.log_dir, "llm_calls.jsonl")

        # åªè¦æœ‰ API Key å°±å°è¯•åˆå§‹åŒ–ï¼Œä¸å†æ£€æŸ¥å‰ç¼€
        if OpenAI and self.api_key:
            try:
                self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
            except Exception as e:
                print(f"Warning: Failed to initialize OpenAI client: {e}")


class LLMService:
    def __init__(self, api_key=None, base_url=None):
        self.api_key = api_key or config.OPENAI_API_KEY
        self.base_url = base_url or config.OPENAI_BASE_URL
        self.client = None

        # é€Ÿç‡é™åˆ¶è·Ÿè¸ª
        self.last_request_time = 0
        self.min_request_interval = 0.5  # æœ€å°è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰

        # Initialize logs directory
        self.log_dir = os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "logs"
        )
        if not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir)
        self.log_file = os.path.join(self.log_dir, "llm_calls.jsonl")

        # åªè¦æœ‰ API Key å°±å°è¯•åˆå§‹åŒ–ï¼Œä¸å†æ£€æŸ¥å‰ç¼€
        if OpenAI and self.api_key:
            try:
                self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
            except Exception as e:
                print(f"Warning: Failed to initialize OpenAI client: {e}")

    def _check_rate_limit(self):
        """æ£€æŸ¥å¹¶å®æ–½é€Ÿç‡é™åˆ¶"""
        now = time.time()
        elapsed = now - self.last_request_time
        if elapsed < self.min_request_interval:
            time.sleep(self.min_request_interval - elapsed)
        self.last_request_time = time.time()

    def _is_rate_limit_error(self, error: Exception) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé€Ÿç‡é™åˆ¶é”™è¯¯ (429)"""
        error_str = str(error).lower()
        return (
            "429" in error_str
            or "rate limit" in error_str
            or "quota" in error_str
            or "exceeded" in error_str
            or "too many requests" in error_str
        )

    def _should_retry_with_backoff(
        self, error: Exception, retry_count: int, max_retries: int = 3
    ) -> tuple:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥é€€é¿é‡è¯•ï¼Œè¿”å› (should_retry, wait_time)
        """
        if not self._is_rate_limit_error(error):
            return False, 0

        if retry_count >= max_retries:
            return False, 0

        # æŒ‡æ•°é€€é¿ï¼š2^retry_count ç§’
        wait_time = min(2**retry_count, 60)  # æœ€å¤šç­‰å¾…60ç§’
        return True, wait_time

    def _log_call(
        self,
        model: str,
        messages: List[Dict[str, str]],
        response: str,
        duration: float,
        status: str = "success",
    ):
        """
        è®°å½•æ¨¡å‹è°ƒç”¨æ—¥å¿—ï¼ˆä¸åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼‰
        """
        # åªè®°å½•æ¶ˆæ¯æ•°é‡å’Œè§’è‰²ï¼Œä¸è®°å½•å†…å®¹
        safe_messages = [{"role": msg["role"]} for msg in messages]

        # è®¡ç®—æ¶ˆæ¯æ‘˜è¦é•¿åº¦
        content_length = sum(len(msg.get("content", "")) for msg in messages)
        response_length = len(response) if response else 0

        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "model": model,
            "message_count": len(messages),
            "content_length": content_length,
            "response_length": response_length,
            "duration_ms": int(duration * 1000),
            "status": status,
        }

        # åªåœ¨å¼€å‘ç¯å¢ƒè®°å½•å“åº”å†…å®¹æ‘˜è¦
        if config.ENV == "development" and status == "success":
            log_entry["response_preview"] = (
                response[:200] + "..." if len(response) > 200 else response
            )

        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        except Exception as e:
            print(f"Failed to write LLM log: {e}")

    def chat_completion(
        self, messages: List[Dict[str, str]], model: str = config.DEFAULT_MODEL
    ) -> str:
        """
        è°ƒç”¨ LLM ç”Ÿæˆå›å¤ï¼Œé‡åˆ° 429 é€Ÿç‡é™åˆ¶æ—¶æŠ›å‡º RateLimitError ä»¥è§¦å‘æ¨¡å‹åˆ‡æ¢ã€‚
        """
        start_time = time.time()
        if not self.client:
            error_msg = f"LLM Client not initialized. API_KEY: {'Set' if self.api_key else 'Missing'}, BASE_URL: {self.base_url}"
            print(f"âŒ {error_msg}")
            raise ValueError(error_msg)

        def _call(extra_body: Dict[str, Any] | None, reasoning_effort: str | None):
            kwargs = {
                "model": model,
                "messages": messages,
                "temperature": 0.7,
                "timeout": 60.0,
            }
            if reasoning_effort:
                kwargs["reasoning_effort"] = reasoning_effort
            if extra_body:
                kwargs["extra_body"] = extra_body
            return self.client.chat.completions.create(**kwargs)

        disable_gemini_thinking = os.getenv("DISABLE_GEMINI_THINKING", "1") != "0"
        reasoning_effort = None
        extra_body = None
        if (
            disable_gemini_thinking
            and ("gemini-2.5-flash" in (model or ""))
            and ("flash-lite" not in (model or ""))
        ):
            reasoning_effort = "none"
            extra_body = {"google": {"thinking_config": {"thinking_budget": 0}}}

        max_retries = (
            1  # é‡åˆ° 429 åªé‡è¯• 1 æ¬¡ï¼Œç„¶åå°±æŠ›å‡º RateLimitError è®©ä¸Šå±‚åˆ‡æ¢æ¨¡å‹
        )
        retry_count = 0
        last_error = None

        while retry_count <= max_retries:
            try:
                print(f"ğŸ“¡ Calling LLM ({model})...")
                response = _call(
                    extra_body=extra_body, reasoning_effort=reasoning_effort
                )
                result = response.choices[0].message.content
                duration = time.time() - start_time
                self._log_call(model, messages, result, duration)
                print(f"âœ… LLM Response received ({duration:.2f}s)")
                return result

            except Exception as e:
                last_error = e

                # å¦‚æœæ˜¯ 429 é€Ÿç‡é™åˆ¶ï¼ŒæŠ›å‡º RateLimitError è®©ä¸Šå±‚åˆ‡æ¢æ¨¡å‹
                if self._is_rate_limit_error(e):
                    duration = time.time() - start_time
                    print(f"âš ï¸ é€Ÿç‡é™åˆ¶è§¦å‘ (429)ï¼Œéœ€è¦åˆ‡æ¢æ¨¡å‹: {str(e)[:80]}")
                    raise RateLimitError(str(e))

                # å¤„ç†å…¶ä»–é”™è¯¯ï¼Œå°è¯•æ— é¢å¤–å‚æ•°é‡è¯•
                if (
                    extra_body is not None or reasoning_effort is not None
                ) and retry_count == 0:
                    retry_count += 1
                    print(f"âš ï¸ é‡è¯•ç§»é™¤ extra_body/reasoning_effort å‚æ•°...")
                    reasoning_effort = None
                    extra_body = None
                    continue

                # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                duration = time.time() - start_time
                error_detail = f"API Error: {str(e)}"
                print(f"âŒ {error_detail}")
                self._log_call(model, messages, error_detail, duration, status="error")
                raise e

        # ç†è®ºä¸Šä¸ä¼šåˆ°è¿™é‡Œ
        duration = time.time() - start_time
        self._log_call(model, messages, str(last_error), duration, status="error")
        raise last_error

    def chat_completion_stream(
        self, messages: List[Dict[str, str]], model: str = config.DEFAULT_MODEL
    ):
        """
        è°ƒç”¨ LLM ç”Ÿæˆæµå¼å›å¤ï¼Œé‡åˆ° 429 é€Ÿç‡é™åˆ¶æ—¶æŠ›å‡º RateLimitError ä»¥è§¦å‘æ¨¡å‹åˆ‡æ¢ã€‚
        """
        start_time = time.time()
        if not self.client:
            error_msg = f"LLM Client not initialized. API_KEY: {'Set' if self.api_key else 'Missing'}, BASE_URL: {self.base_url}"
            print(f"âŒ {error_msg}")
            raise ValueError(error_msg)

        def _call_stream(
            extra_body: Dict[str, Any] | None, reasoning_effort: str | None
        ):
            kwargs = {
                "model": model,
                "messages": messages,
                "temperature": 0.7,
                "timeout": 60.0,
                "stream": True,
            }
            if reasoning_effort:
                kwargs["reasoning_effort"] = reasoning_effort
            if extra_body:
                kwargs["extra_body"] = extra_body
            return self.client.chat.completions.create(**kwargs)

        disable_gemini_thinking = os.getenv("DISABLE_GEMINI_THINKING", "1") != "0"
        reasoning_effort = None
        extra_body = None
        if (
            disable_gemini_thinking
            and ("gemini-2.5-flash" in (model or ""))
            and ("flash-lite" not in (model or ""))
        ):
            reasoning_effort = "none"
            extra_body = {"google": {"thinking_config": {"thinking_budget": 0}}}

        max_retries = (
            1  # é‡åˆ° 429 åªé‡è¯• 1 æ¬¡ï¼Œç„¶åå°±æŠ›å‡º RateLimitError è®©ä¸Šå±‚åˆ‡æ¢æ¨¡å‹
        )
        retry_count = 0
        last_error = None

        while retry_count <= max_retries:
            try:
                full_response = ""
                print(f"ğŸ“¡ Calling LLM Stream ({model})...")
                stream = _call_stream(
                    extra_body=extra_body, reasoning_effort=reasoning_effort
                )

                for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        yield content

                duration = time.time() - start_time
                self._log_call(model, messages, full_response, duration)
                print(f"âœ… LLM Stream completed ({duration:.2f}s)")
                return

            except Exception as e:
                last_error = e

                # å¦‚æœæ˜¯ 429 é€Ÿç‡é™åˆ¶ï¼ŒæŠ›å‡º RateLimitError è®©ä¸Šå±‚åˆ‡æ¢æ¨¡å‹
                if self._is_rate_limit_error(e):
                    duration = time.time() - start_time
                    print(f"âš ï¸ é€Ÿç‡é™åˆ¶è§¦å‘ (429)ï¼Œéœ€è¦åˆ‡æ¢æ¨¡å‹: {str(e)[:80]}")
                    raise RateLimitError(str(e))

                # å¤„ç†å…¶ä»–é”™è¯¯ï¼Œå°è¯•æ— é¢å¤–å‚æ•°é‡è¯•
                if (
                    extra_body is not None or reasoning_effort is not None
                ) and retry_count == 0:
                    retry_count += 1
                    print(f"âš ï¸ é‡è¯•ç§»é™¤ extra_body/reasoning_effort å‚æ•°...")
                    reasoning_effort = None
                    extra_body = None
                    continue

                # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                duration = time.time() - start_time
                error_detail = f"API Stream Error: {str(e)}"
                print(f"âŒ {error_detail}")
                self._log_call(model, messages, error_detail, duration, status="error")
                raise e

        # ç†è®ºä¸Šä¸ä¼šåˆ°è¿™é‡Œ
        duration = time.time() - start_time
        self._log_call(model, messages, str(last_error), duration, status="error")
        raise last_error
