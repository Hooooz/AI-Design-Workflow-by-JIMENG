import os
import sys
import time
import json
from datetime import datetime
from typing import List, Dict, Any

# Add current directory to path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

import config


class RateLimitError(Exception):
    """é€Ÿç‡é™åˆ¶é”™è¯¯ï¼Œéœ€è¦åˆ‡æ¢æ¨¡å‹"""

    def __init__(self, message="Rate limit exceeded", retry_after: int = None):
        self.message = message
        self.retry_after = retry_after
        super().__init__(message)


class LLMService:
    def __init__(self, api_key=None, base_url=None):
        self.api_key = api_key or config.OPENAI_API_KEY
        self.base_url = base_url or config.OPENAI_BASE_URL
        self.client = None

        # Initialize logs directory
        self.log_dir = os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "logs"
        )
        if not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir)
        self.log_file = os.path.join(self.log_dir, "llm_calls.jsonl")

        # åªè¦æœ‰ API Key å°±å°è¯•åˆå§‹åŒ–ï¼Œä¸å†æ£€æŸ¥å‰ç¼€
        if OpenAI and self.api_key:
            try:
                self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
            except Exception as e:
                print(f"Warning: Failed to initialize OpenAI client: {e}")


class LLMService:
    def __init__(self, api_key=None, base_url=None):
        self.api_key = api_key or config.OPENAI_API_KEY
        self.base_url = base_url or config.OPENAI_BASE_URL
        self.client = None

        # é€Ÿç‡é™åˆ¶è·Ÿè¸ª
        self.last_request_time = 0
        self.min_request_interval = 0.5  # æœ€å°è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰

        # Initialize logs directory
        self.log_dir = os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "logs"
        )
        if not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir)
        self.log_file = os.path.join(self.log_dir, "llm_calls.jsonl")

        # åªè¦æœ‰ API Key å°±å°è¯•åˆå§‹åŒ–ï¼Œä¸å†æ£€æŸ¥å‰ç¼€
        if OpenAI and self.api_key:
            try:
                self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
            except Exception as e:
                print(f"Warning: Failed to initialize OpenAI client: {e}")

    def _check_rate_limit(self):
        """æ£€æŸ¥å¹¶å®æ–½é€Ÿç‡é™åˆ¶"""
        now = time.time()
        elapsed = now - self.last_request_time
        if elapsed < self.min_request_interval:
            time.sleep(self.min_request_interval - elapsed)
        self.last_request_time = time.time()

    def _is_rate_limit_error(self, error: Exception) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé€Ÿç‡é™åˆ¶é”™è¯¯ (429)"""
        error_str = str(error).lower()
        return (
            "429" in error_str
            or "rate limit" in error_str
            or "quota" in error_str
            or "exceeded" in error_str
            or "too many requests" in error_str
        )

    def _should_retry_with_backoff(
        self, error: Exception, retry_count: int, max_retries: int = 3
    ) -> tuple:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥é€€é¿é‡è¯•ï¼Œè¿”å› (should_retry, wait_time)
        """
        if not self._is_rate_limit_error(error):
            return False, 0

        if retry_count >= max_retries:
            return False, 0

        # æŒ‡æ•°é€€é¿ï¼š2^retry_count ç§’
        wait_time = min(2**retry_count, 60)  # æœ€å¤šç­‰å¾…60ç§’
        return True, wait_time

    def _log_call(
        self,
        model: str,
        messages: List[Dict[str, str]],
        response: str,
        duration: float,
        status: str = "success",
    ):
        """
        è®°å½•æ¨¡å‹è°ƒç”¨æ—¥å¿—ï¼ˆä¸åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼‰
        """
        # åªè®°å½•æ¶ˆæ¯æ•°é‡å’Œè§’è‰²ï¼Œä¸è®°å½•å†…å®¹
        safe_messages = [{"role": msg["role"]} for msg in messages]

        # è®¡ç®—æ¶ˆæ¯æ‘˜è¦é•¿åº¦
        content_length = sum(len(msg.get("content", "")) for msg in messages)
        response_length = len(response) if response else 0

        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "model": model,
            "message_count": len(messages),
            "content_length": content_length,
            "response_length": response_length,
            "duration_ms": int(duration * 1000),
            "status": status,
        }

        # åªåœ¨å¼€å‘ç¯å¢ƒè®°å½•å“åº”å†…å®¹æ‘˜è¦
        if config.ENV == "development" and status == "success":
            log_entry["response_preview"] = (
                response[:200] + "..." if len(response) > 200 else response
            )

        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        except Exception as e:
            print(f"Failed to write LLM log: {e}")

    def chat_completion(self, messages: List[Dict[str, str]], model: str = None) -> str:
        """
        è°ƒç”¨ LLM ç”Ÿæˆå›å¤ï¼Œæ”¯æŒè‡ªåŠ¨æ¨¡å‹é™çº§ (Failover)ã€‚
        ç­–ç•¥ï¼šä¼˜å…ˆå°è¯•æŒ‡å®šæ¨¡å‹ï¼Œå¤±è´¥åæŒ‰ä¼˜å…ˆçº§åˆ—è¡¨å°è¯•å…¶ä»–æ¨¡å‹ã€‚
        """
        start_time = time.time()
        if not self.client:
            error_msg = f"LLM Client not initialized. API_KEY: {'Set' if self.api_key else 'Missing'}, BASE_URL: {self.base_url}"
            print(f"âŒ {error_msg}")
            raise ValueError(error_msg)

        # ç¡®å®šæ¨¡å‹å°è¯•åºåˆ—
        # 1. é»˜è®¤æƒ…å†µï¼šä½¿ç”¨ config ä¸­çš„ä¼˜å…ˆçº§åˆ—è¡¨
        # 2. æŒ‡å®šæƒ…å†µï¼šä¼˜å…ˆå°è¯•æŒ‡å®šæ¨¡å‹ï¼Œå¤±è´¥åå°è¯•åˆ—è¡¨ä¸­å‰©ä½™çš„æ¨¡å‹
        candidate_models = list(config.MODEL_PRIORITY_LIST)
        requested_model = model or config.DEFAULT_MODEL

        # å¦‚æœè¯·æ±‚çš„æ¨¡å‹ä¸åœ¨åˆ—è¡¨ä¸­ï¼ŒæŠŠå®ƒåŠ åˆ°æœ€å‰é¢
        if requested_model not in candidate_models:
            candidate_models.insert(0, requested_model)
        else:
            # å¦‚æœåœ¨åˆ—è¡¨ä¸­ï¼Œç¡®ä¿å®ƒæ’åœ¨ç¬¬ä¸€ä¸ªï¼Œå¹¶ä¿æŒåˆ—è¡¨å…¶ä½™éƒ¨åˆ†çš„ç›¸å¯¹é¡ºåº
            candidate_models.remove(requested_model)
            candidate_models.insert(0, requested_model)

        last_error = None

        for current_model in candidate_models:
            try:
                # å†…éƒ¨å‡½æ•°ï¼šæ‰§è¡Œå•ä¸ªæ¨¡å‹çš„è°ƒç”¨ï¼ˆå«å‚æ•°é‡è¯•é€»è¾‘ï¼‰
                def _call(
                    extra_body: Dict[str, Any] | None, reasoning_effort: str | None
                ):
                    kwargs = {
                        "model": current_model,
                        "messages": messages,
                        "temperature": 0.7,
                        "timeout": 60.0,
                    }
                    if reasoning_effort:
                        kwargs["reasoning_effort"] = reasoning_effort
                    if extra_body:
                        kwargs["extra_body"] = extra_body
                    return self.client.chat.completions.create(**kwargs)

                # Gemini Thinking å‚æ•°é…ç½®
                disable_gemini_thinking = (
                    os.getenv("DISABLE_GEMINI_THINKING", "1") != "0"
                )
                reasoning_effort = None
                extra_body = None
                if (
                    disable_gemini_thinking
                    and ("gemini-2.5-flash" in current_model)
                    and ("flash-lite" not in current_model)
                ):
                    reasoning_effort = "none"
                    extra_body = {"google": {"thinking_config": {"thinking_budget": 0}}}

                # å•ä¸ªæ¨¡å‹çš„é‡è¯•å¾ªç¯ï¼ˆå¤„ç†å‚æ•°é”™è¯¯ç­‰ï¼‰
                max_retries = 1
                retry_count = 0

                while retry_count <= max_retries:
                    try:
                        print(f"ğŸ“¡ Calling LLM ({current_model})...")
                        response = _call(
                            extra_body=extra_body, reasoning_effort=reasoning_effort
                        )

                        raw_content = response.choices[0].message.content
                        if not raw_content:
                            raise ValueError(
                                f"Model {current_model} returned empty response."
                            )

                        result = str(raw_content).strip()

                        if len(result) < 5:
                            raise ValueError(
                                f"Model {current_model} returned too short response."
                            )

                        duration = time.time() - start_time
                        self._log_call(current_model, messages, result, duration)
                        print(
                            f"âœ… LLM Response received from {current_model} ({duration:.2f}s)"
                        )
                        return result

                    except Exception as e:
                        # å‚æ•°é”™è¯¯é‡è¯•é€»è¾‘
                        if (
                            extra_body is not None or reasoning_effort is not None
                        ) and retry_count == 0:
                            retry_count += 1
                            print(
                                f"âš ï¸ [{current_model}] å‚æ•°ä¸å…¼å®¹ï¼Œç§»é™¤ extra_body é‡è¯•..."
                            )
                            reasoning_effort = None
                            extra_body = None
                            continue
                        raise e  # æŠ›å‡ºç»™å¤–å±‚å¤„ç†ï¼ˆè¿›è¡Œæ¨¡å‹åˆ‡æ¢ï¼‰

            except Exception as e:
                last_error = e
                duration = time.time() - start_time
                error_msg = str(e).lower()

                # åˆ¤æ–­æ˜¯å¦å€¼å¾—åˆ‡æ¢æ¨¡å‹
                # å¢åŠ äº† "empty/invalid response" çš„æ£€æµ‹
                should_failover = any(
                    code in error_msg
                    for code in [
                        "404",
                        "429",
                        "500",
                        "not found",
                        "rate limit",
                        "overloaded",
                        "empty/invalid",
                    ]
                )

                if should_failover:
                    print(
                        f"âš ï¸ Model {current_model} failed: {str(e)[:100]}... -> Trying next model"
                    )
                    continue  # Try next model

                # å¦‚æœæ˜¯å…¶ä»–ä¸¥é‡é”™è¯¯ï¼ˆå¦‚è®¤è¯å¤±è´¥ï¼‰ï¼Œç›´æ¥ç»ˆæ­¢
                print(f"âŒ Unrecoverable error on {current_model}: {e}")
                raise e

        # æ‰€æœ‰æ¨¡å‹éƒ½å°è¯•å¤±è´¥
        print("âŒ All candidate models failed.")
        raise last_error

    def chat_completion_stream(self, messages: List[Dict[str, str]], model: str = None):
        """
        è°ƒç”¨ LLM ç”Ÿæˆæµå¼å›å¤ï¼Œæ”¯æŒè‡ªåŠ¨æ¨¡å‹é™çº§ (Failover)ã€‚
        """
        start_time = time.time()
        if not self.client:
            raise ValueError("LLM Client not initialized")

        # ç¡®å®šæ¨¡å‹å°è¯•åºåˆ— (åŒä¸Š)
        candidate_models = list(config.MODEL_PRIORITY_LIST)
        requested_model = model or config.DEFAULT_MODEL
        if requested_model not in candidate_models:
            candidate_models.insert(0, requested_model)
        else:
            candidate_models.remove(requested_model)
            candidate_models.insert(0, requested_model)

        last_error = None

        for current_model in candidate_models:
            try:

                def _call_stream(extra_body, reasoning_effort):
                    kwargs = {
                        "model": current_model,
                        "messages": messages,
                        "temperature": 0.7,
                        "timeout": 60.0,
                        "stream": True,
                    }
                    if reasoning_effort:
                        kwargs["reasoning_effort"] = reasoning_effort
                    if extra_body:
                        kwargs["extra_body"] = extra_body
                    return self.client.chat.completions.create(**kwargs)

                # Gemini å‚æ•°é…ç½®
                reasoning_effort = None
                extra_body = None
                # ... (åŒæ ·çš„å‚æ•°é…ç½®é€»è¾‘) ...

                # æ‰§è¡Œæµå¼è°ƒç”¨
                print(f"ğŸ“¡ Calling LLM Stream ({current_model})...")
                full_response = ""

                # æ³¨æ„ï¼šæµå¼è°ƒç”¨åœ¨è¿™é‡Œåªæ˜¯å»ºç«‹è¿æ¥ï¼Œå¦‚æœåœ¨è¿­ä»£è¿‡ç¨‹ä¸­æŠ¥é”™ï¼Œå¾ˆéš¾åœ¨è¿™é‡Œæ•è·å¹¶åˆ‡æ¢æ¨¡å‹
                # æ‰€ä»¥æˆ‘ä»¬ä¸»è¦æ•è·å»ºç«‹è¿æ¥æ—¶çš„é”™è¯¯
                stream = _call_stream(extra_body, reasoning_effort)

                for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        yield content

                # æˆåŠŸå®Œæˆ
                duration = time.time() - start_time
                self._log_call(current_model, messages, full_response, duration)
                print(f"âœ… LLM Stream completed from {current_model} ({duration:.2f}s)")
                return

            except Exception as e:
                last_error = e
                print(f"âš ï¸ Model {current_model} stream failed: {str(e)[:100]}")
                # ç®€å•åˆ¤æ–­æ˜¯å¦ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                continue

        print("âŒ All candidate models failed for stream.")
        raise last_error
